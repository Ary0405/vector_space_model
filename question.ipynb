{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aryansethia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aryansethia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aryansethia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/aryansethia/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # Case folding\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Stop word removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read Documents from the folder\n",
    "def readDocuments(folderName):\n",
    "    documents = defaultdict(list)\n",
    "    try:\n",
    "        for filename in os.listdir(folderName):\n",
    "            with open(folderName + \"/\" + filename, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "                text_tokens = preprocess(text)\n",
    "                documents[filename] = text_tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading documents: {e}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Find all unique words\n",
    "def findUniqueWords(documents):\n",
    "    unique_words = set()\n",
    "    for document in documents.values():\n",
    "        for word in document:\n",
    "            unique_words.add(word)\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate tf Weight for documents\n",
    "def calculatetfWeight(tf):\n",
    "    return 1 + math.log10(tf) if tf > 0 else 0\n",
    "\n",
    "# Step 4: Calculate idf Weight for query\n",
    "def calculateidfWeight(df, N):\n",
    "    return math.log10(N / df) if df > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create Posting List (stores log(tf) for documents)\n",
    "def createPostingList(documents, unique_words):\n",
    "    posting_list = {}\n",
    "    document_frequencies = defaultdict(int)\n",
    "    N = len(documents)\n",
    "\n",
    "    # Calculate document frequencies\n",
    "    for word in unique_words:\n",
    "        for text_tokens in documents.values():\n",
    "            if word in text_tokens:\n",
    "                document_frequencies[word] += 1\n",
    "    \n",
    "    # Build the posting list\n",
    "    for word in unique_words:\n",
    "        posting_list[word] = []\n",
    "        for doc_name, text_tokens in documents.items():\n",
    "            term_freq = text_tokens.count(word)\n",
    "            if term_freq > 0:\n",
    "                # Store log(tf) for document\n",
    "                posting_list[word].append((doc_name, calculatetfWeight(term_freq)))\n",
    "\n",
    "    # Save the posting list to a text file in the desired format\n",
    "    try:\n",
    "        with open(\"posting_list.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for word, postings in posting_list.items():\n",
    "                df = document_frequencies[word]  # Get actual document frequency value\n",
    "                # Write in the format \"term\", df: [(doc: \"logtf\"), ...]\n",
    "                f.write(f'\"{word}\", {df}: [')\n",
    "                f.write(\", \".join([f'(\"{doc}\", \"{logtf:.6f}\")' for doc, logtf in postings]))\n",
    "                f.write(\"]\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to file: {e}\")\n",
    "\n",
    "                \n",
    "    return posting_list, document_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Normalize Vectors using Cosine Normalization\n",
    "def normalize_vector(vector):\n",
    "    norm = math.sqrt(sum(weight ** 2 for weight in vector.values()))\n",
    "    if norm == 0:\n",
    "        return vector\n",
    "    return {term: weight / norm for term, weight in vector.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Compute Cosine Similarity\n",
    "def compute_cosine_similarity(query_vector, doc_vector):\n",
    "    dot_product = sum(query_vector[term] * doc_vector.get(term, 0) for term in query_vector)\n",
    "    return dot_product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Rank documents based on cosine similarity\n",
    "def rank_documents(documents, query, posting_list, document_frequencies, unique_words):\n",
    "    N = len(documents)\n",
    "    query_tokens = preprocess(query)\n",
    "    unique_words_query = set(query_tokens)\n",
    "    unique_words= unique_words.union(unique_words_query)\n",
    "    query_vector = {}\n",
    "\n",
    "    # Calculate query tf-idf weights (ltc scheme)\n",
    "    for word in unique_words:\n",
    "        tf = query_tokens.count(word)\n",
    "        # Get df from doc_frequency based on word, default to 0 if word not found\n",
    "        df = document_frequencies.get(word, 0)\n",
    "        if df > 0:\n",
    "            idf = calculateidfWeight(df, N) \n",
    "            query_vector[word] = calculatetfWeight(tf) * idf\n",
    "\n",
    "    # Normalize the query vector\n",
    "    query_vector = normalize_vector(query_vector)\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    similarities = {}\n",
    "    for doc_name in documents.keys():\n",
    "        doc_vector = {}\n",
    "        for word in unique_words:\n",
    "        # Get the posting list for the current word\n",
    "            posting = posting_list.get(word, [])\n",
    "            # Check if the document has the word\n",
    "        \n",
    "            for doc, log_tf in posting:\n",
    "                if doc == doc_name:\n",
    "                # Set idf = 1 for documents in lnc scheme\n",
    "                    doc_vector[word] = log_tf * 1  # Assign the log_tf value\n",
    "        \n",
    "            #If the word is not found, you can explicitly set its value to 0\n",
    "            if word not in doc_vector:\n",
    "                doc_vector[word] = 0\n",
    "\n",
    "        # Normalize the document vector\n",
    "        doc_vector = normalize_vector(doc_vector)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarity = compute_cosine_similarity(query_vector, doc_vector)\n",
    "        similarities[doc_name] = similarity\n",
    "\n",
    "    # Sort documents by similarity and return top 10\n",
    "    ranked_docs = sorted(similarities.items(), key=lambda item: (-item[1], item[0]))\n",
    "    return ranked_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Documents by Relevance for query 1:\n",
      "zomato.txt: 0.2146\n",
      "swiggy.txt: 0.1310\n",
      "instagram.txt: 0.0605\n",
      "messenger.txt: 0.0592\n",
      "youtube.txt: 0.0585\n",
      "Discord.txt: 0.0533\n",
      "bing.txt: 0.0518\n",
      "paypal.txt: 0.0471\n",
      "reddit.txt: 0.0441\n",
      "flipkart.txt: 0.0407\n",
      "\n",
      "\n",
      "Ranked Documents by Relevance for query 2:\n",
      "shakespeare.txt: 0.1201\n",
      "levis.txt: 0.0241\n",
      "Adobe.txt: 0.0227\n",
      "google.txt: 0.0207\n",
      "nike.txt: 0.0192\n",
      "zomato.txt: 0.0177\n",
      "huawei.txt: 0.0137\n",
      "skype.txt: 0.0117\n",
      "blackberry.txt: 0.0109\n",
      "Dell.txt: 0.0108\n"
     ]
    }
   ],
   "source": [
    "folderName = \"Corpus\"\n",
    "query = \"Developing your Zomato business account and profile is a great way to boost your restaurantâ€™s online reputation\"\n",
    "query2 = \"Warwickshire, came from an ancient family and was the heiress to some land\"\n",
    "# Read documents and create posting list\n",
    "documents = readDocuments(folderName)\n",
    "unique_words = findUniqueWords(documents)\n",
    "posting_list, document_frequencies = createPostingList(documents, unique_words)\n",
    "# Rank documents by cosine similarity\n",
    "ranked_docs = rank_documents(documents, query, posting_list, document_frequencies, unique_words)\n",
    "ranked_docs2 = rank_documents(documents, query2, posting_list, document_frequencies, unique_words)\n",
    "print(\"Ranked Documents by Relevance for query 1:\")\n",
    "for doc_name, score in ranked_docs:\n",
    "    print(f\"{doc_name}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Ranked Documents by Relevance for query 2:\")\n",
    "for doc_name, score in ranked_docs2:\n",
    "    print(f\"{doc_name}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
